{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy import argmax, array, log\n",
    "from numpy.random import rand, randn, choice\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_normal_dist(n_samples_=10, mu=0, variance=1):\n",
    "    return (sqrt(variance)*randn(n_samples_))+mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, no_arms, reward_var = 1):\n",
    "        self.no_arms = no_arms\n",
    "        self.reward_var = reward_var\n",
    "        # Initialises arm-means with normal distribution\n",
    "        self.q_stars = get_samples_from_normal_dist(no_arms)\n",
    "        self.best_arm = argmax(self.q_stars)\n",
    "    def get_best_arm(self):\n",
    "        return self.best_arm\n",
    "    def get_rewards(self, arm, no_samples=1):\n",
    "        # Returns rewards from normal distribution with\n",
    "        # mean = arm_mean and variance = reward_var\n",
    "        return get_samples_from_normal_dist(no_samples, mu=self.q_stars[arm], variance=self.reward_var)\n",
    "    def plot(self, no_samples=2000, save_path=None):\n",
    "        reward_set = []\n",
    "        for arm_idx in range(self.no_arms):\n",
    "            reward_set.append(self.get_rewards(arm_idx,no_samples))\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.violinplot(dataset=reward_set,showmeans=True)\n",
    "        plt.xticks(list(range(1,self.no_arms+1)),list(range(self.no_arms)))\n",
    "        plt.xlabel(\"Arm\")\n",
    "        plt.ylabel(\"Reward distribution\")\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedy(Bandit):\n",
    "    def __init__(self, no_arms, reward_var = 1, epsilon_=0):\n",
    "        super(EGreedy,self).__init__(no_arms, reward_var)\n",
    "        self.epsilon = epsilon_\n",
    "        self.current_best_arm = (-1, 0)\n",
    "        self.q_arms = defaultdict(float)\n",
    "        self.arms_count = defaultdict(int)\n",
    "        self.time = 0\n",
    "        \n",
    "    def pick_best_arm(self):\n",
    "        pick_determiner = rand()\n",
    "        if pick_determiner<self.epsilon or (self.current_best_arm[0]==-1):\n",
    "            return choice(self.no_arms)\n",
    "        return self.current_best_arm[0]\n",
    "    \n",
    "    def update_current_best_arm(self, arm):\n",
    "        if self.current_best_arm[0] == arm or (self.current_best_arm[1]< self.q_arms[arm]):\n",
    "            self.current_best_arm = (arm, self.q_arms[arm])\n",
    "    \n",
    "    def update_values(self, arm, reward):\n",
    "        # Update self.arms_count\n",
    "        self.arms_count[arm] += 1\n",
    "        # Update q_arms\n",
    "        self.q_arms[arm] += (reward - self.q_arms[arm]) / self.arms_count[arm]\n",
    "        # Update self.current_best_arm\n",
    "        self.update_current_best_arm(arm)\n",
    "        \n",
    "    def run(self, time_instances=1000):\n",
    "        rewards_obtained = []\n",
    "        is_opt_action = []\n",
    "        arms_pulled = []\n",
    "        while self.time < time_instances:\n",
    "            self.time += 1\n",
    "            chosen_arm = self.pick_best_arm()\n",
    "            reward = self.get_rewards(chosen_arm)[0]\n",
    "            rewards_obtained.append(reward)\n",
    "            is_opt_action.append(1 if chosen_arm==self.best_arm else 0)\n",
    "            self.update_values(chosen_arm, reward)\n",
    "        return rewards_obtained, is_opt_action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(EGreedy):\n",
    "    def __init__(self, no_arms, reward_var = 1, epsilon_=0, ucb_confidence_ = 0.1):\n",
    "        super(UCB,self).__init__(no_arms, reward_var, epsilon_)\n",
    "        self.ucb_confidence = ucb_confidence_\n",
    "        \n",
    "    def update_current_best_arm(self, arm):\n",
    "        ucb_estimations = []\n",
    "        for arm_ in range(self.no_arms):\n",
    "            ucb_estimations.append(self.q_arms[arm_] + \\\n",
    "            self.ucb_confidence * sqrt(log(self.time + 1) / (self.arms_count[arm_] + 1e-5)))\n",
    "        current_max_arm = argmax(ucb_estimations)\n",
    "        self.current_best_arm = (current_max_arm,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, no_arms, time_instances, no_runs,\n",
    "                 reward_variance_list, epsilon_list, ucb_conf_list,\n",
    "                 plot_save_path):\n",
    "        self.no_arms = no_arms\n",
    "        self.time_instances = time_instances\n",
    "        self.no_runs = no_runs\n",
    "        self.reward_variance_list = reward_variance_list\n",
    "        self.epsilon_list = epsilon_list\n",
    "        self.ucb_conf_list = ucb_conf_list\n",
    "        self.plot_save_path = plot_save_path\n",
    "       \n",
    "    @staticmethod\n",
    "    def add_plots(data, legend_label, y_label, title_):\n",
    "        plt.plot(data, label=legend_label)\n",
    "        plt.title(title_)\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend() \n",
    "        \n",
    "    def simulate_single_exp(self, reward_var = 1, epsilon=0, ucb_confidence=None):\n",
    "        avg_rewards_coll = []\n",
    "        rewards_coll = []\n",
    "        is_opt_coll = []\n",
    "        for _ in trange(self.no_runs):\n",
    "            if ucb_confidence:\n",
    "                selected_bandit = UCB(self.no_arms, reward_var, epsilon_=epsilon, ucb_confidence_= ucb_confidence)\n",
    "            else:\n",
    "                selected_bandit = EGreedy(self.no_arms, reward_var, epsilon_=epsilon)\n",
    "            rewards, is_opt = selected_bandit.run(self.time_instances)\n",
    "            rewards_coll.append(rewards)\n",
    "            is_opt_coll.append(is_opt)\n",
    "        return array(rewards_coll).mean(axis=0), array(is_opt_coll).mean(axis=0)*100\n",
    "    \n",
    "    def run(self):\n",
    "        for reward_variance in self.reward_variance_list:\n",
    "            print(\"===============================================\")\n",
    "            \n",
    "            # -------------- Simulating --------------\n",
    "            print(f\"Simultion for Reward Variance:{reward_variance}\")\n",
    "            average_reward_collection = []\n",
    "            optimal_action_collection = []\n",
    "            for eps in self.epsilon_list:\n",
    "                print(f\"Simulating for epsilon {eps}\")\n",
    "                rew_coll, per_opt_coll = self.simulate_single_exp(reward_var=reward_variance,\n",
    "                                                                  epsilon=eps)\n",
    "                average_reward_collection.append((rew_coll,'epsilon = %.02f' % (eps)))\n",
    "                optimal_action_collection.append((per_opt_coll,'epsilon = %.02f' % (eps)))\n",
    "            for ucb_conf in self.ucb_conf_list:\n",
    "                print(f\"Simulating for UCB confidence {ucb_conf}\")\n",
    "                rew_coll, per_opt_coll = self.simulate_single_exp(reward_var=reward_variance,\n",
    "                                                                  epsilon=0,\n",
    "                                                                  ucb_confidence=ucb_conf)\n",
    "                average_reward_collection.append((rew_coll,'UCB = %.02f' % (ucb_conf)))\n",
    "                optimal_action_collection.append((per_opt_coll,'UCB = %.02f' % (ucb_conf)))\n",
    "                \n",
    "            # -------------- Plotting --------------\n",
    "            plt.figure(figsize=(10, 20))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            for rewards, legend_label in average_reward_collection:\n",
    "                self.add_plots(rewards,legend_label,'average reward', \"Awerage Reward vs Steps\")\n",
    "            plt.subplot(2, 1, 2)\n",
    "            for counts, legend_label in optimal_action_collection:\n",
    "                self.add_plots(counts,legend_label,'% optimal action', \"Optimal Action Percentage vs Steps\")\n",
    "            if self.plot_save_path:\n",
    "                plt.savefig(self.plot_save_path.format(f\"variance_{reward_variance}\"))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit with reward-variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_var1 = Bandit(10)\n",
    "bandit_var1.plot()\n",
    "print(f\"Optimal Arm is {bandit_var1.get_best_arm()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit with reward-variance = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_var10 = Bandit(10, reward_var=10)\n",
    "bandit_var10.plot()\n",
    "print(f\"Optimal Arm is {bandit_var10.get_best_arm()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average reward and % optimal action plots for multiple variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(no_arms=10, time_instances=1000, no_runs=2000,\n",
    "                        reward_variance_list= [1,10],\n",
    "                        epsilon_list=[0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.9],\n",
    "                        ucb_conf_list=[0.1, 0.5, 1, 2, 3, 5, 10],\n",
    "                        plot_save_path=\"10_arms_bandit_{}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study and Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Comparison between different epsilon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(no_arms=10, time_instances=1000, no_runs=2000,\n",
    "                        reward_variance_list= [1],\n",
    "                        epsilon_list=[0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.9],\n",
    "                        ucb_conf_list=[],\n",
    "                        plot_save_path=None)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Comparison between different UCB confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(no_arms=10, time_instances=1000, no_runs=2000,\n",
    "                        reward_variance_list= [1],\n",
    "                        epsilon_list=[],\n",
    "                        ucb_conf_list=[0.1, 0.5, 1, 2, 3, 5, 10],\n",
    "                        plot_save_path=None)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Comparison between E-Greedy and UCB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(no_arms=10, time_instances=1000, no_runs=2000,\n",
    "                        reward_variance_list= [1],\n",
    "                        epsilon_list=[0],\n",
    "                        ucb_conf_list=[2],\n",
    "                        plot_save_path=\"sample_{}.png\")\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Comparison between different reward-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(no_arms=10, time_instances=1000, no_runs=2000,\n",
    "                        reward_variance_list= [1,10],\n",
    "                        epsilon_list=[0],\n",
    "                        ucb_conf_list=[],\n",
    "                        plot_save_path=None)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
