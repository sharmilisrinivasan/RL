{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation of policy for given Gird World problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![state transtion probabilities](StateTransitionProbabilities.png \"State Transition Probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When two optimal directions are possible, both directions are chosen with a probability **0.5**; Else optimal direction is chosen with probability **1**.\n",
    "- Given optimal direction is chosen,\n",
    "    - Probability of moving in desired direction = **0.7**\n",
    "    - All other non-optimal directions are equi-probable = **0.1**\n",
    "\n",
    "#### Sample Calculations\n",
    "##### State-1\n",
    "- `P(T/s1)`\n",
    "    \n",
    "    `= P(Left/s1) * P(T/s1, Left)`  \n",
    "    \n",
    "    `= 1 * 0.7  = 0.7`  # Since only one optimal direction\n",
    "    \n",
    "    \n",
    "- `P(s1/s1) = P(s2/s1) = P(s5/s1) = 0.1` # All non-optimal actions are equiprobable and `P(next-state/ non-optimal action) = 1`\n",
    "\n",
    "Similarly for states s2, s4, s7, s8, s11, s13, 14\n",
    "\n",
    "##### State-3\n",
    "- `P(s2/s3)`\n",
    "\n",
    "    `= (P(Left/s3) * P(s2/ s3, Left)) + (P(Down/s3) * P(s2/ s3, Down))`  # Since 2 equi-probable optimal actions\n",
    "    \n",
    "    `= (0.5 * 0.7) + (0.5 * 0.1) = 0.35 + 0.05 = 0.4`\n",
    "  \n",
    "- `P(s7/s3)`\n",
    "\n",
    "    `= (P(Down/s3) * P(s7/ s3, Down)) + (P(Left/s3) * P(s7/ s3, Left))`\n",
    "    \n",
    "    `= (0.5 * 0.7) + (0.5 * 0.1) = 0.35 + 0.05 = 0.4` # Same reason as above\n",
    "    \n",
    "- `P(s3/s3) = 0.2`  #  Remaining probability\n",
    "\n",
    "Similarly for state s12\n",
    "\n",
    "##### State-5\n",
    "- Using same 2 equi-probable optimal action logic as in state s3,\n",
    "    \n",
    "    `P(s4/s5) = P(s1/s5) = 0.4`\n",
    "    \n",
    "- Other two states take equal probability in remaining probability as,\n",
    "    \n",
    "    `P(s6/s5) = P(s9/s5) = 0.1`\n",
    "    \n",
    "Similarly for states s6, s9, s10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above policy in the form of dictionary\n",
    "policy = {\n",
    "    \"s1\":[(\"X\",0.7),(\"s1\",0.1),(\"s2\",0.1),(\"s5\",0.1)],\n",
    "    \"s2\":[(\"s1\",0.7),(\"s2\",0.1),(\"s3\",0.1),(\"s6\",0.1)],\n",
    "    \"s3\":[(\"s2\",0.4),(\"s3\",0.2),(\"s7\",0.4)],\n",
    "    \"s4\":[(\"s4\",0.1),(\"X\",0.7),(\"s5\",0.1),(\"s8\",0.1)],\n",
    "    \"s5\":[(\"s4\",0.4),(\"s1\",0.4),(\"s6\",0.1),(\"s9\",0.1)],\n",
    "    \"s6\":[(\"s5\",0.4),(\"s2\",0.1),(\"s7\",0.1),(\"s10\",0.4)],\n",
    "    \"s7\":[(\"s6\",0.1),(\"s3\",0.1),(\"s7\",0.1),(\"s11\",0.7)],\n",
    "    \"s8\":[(\"s8\",0.1),(\"s4\",0.7),(\"s9\",0.1),(\"s12\",0.1)],\n",
    "    \"s9\":[(\"s8\",0.1),(\"s5\",0.4),(\"s10\",0.4),(\"s13\",0.1)],\n",
    "    \"s10\":[(\"s9\",0.1),(\"s6\",0.1),(\"s11\",0.4),(\"s14\",0.4)],\n",
    "    \"s11\":[(\"s10\",0.1),(\"s7\",0.1),(\"s11\",0.1),(\"X\",0.7)],\n",
    "    \"s12\":[(\"s12\",0.2),(\"s8\",0.4),(\"s13\",0.4)],\n",
    "    \"s13\":[(\"s12\",0.1),(\"s9\",0.1),(\"s14\",0.7),(\"s13\",0.1)],\n",
    "    \"s14\":[(\"s13\",0.1),(\"s10\",0.1),(\"X\",0.7),(\"s14\",0.1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, policy_dict, terminal_state_key = \"X\"):\n",
    "        self.policy = policy_dict\n",
    "        self.non_terminal_states_cnt = len(self.policy)\n",
    "        self.terminal_state_key = terminal_state_key\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_reward(episode_seq):\n",
    "        # All non-terminal states get reward 1 and terminal states get reward 0; gamma (discount factor) = 1\n",
    "        return list(zip(episode_seq, list(range((-1)*(len(episode_seq)-2),1))))\n",
    "        \n",
    "    def generate_episode(self, reward = True):\n",
    "        \n",
    "        # generate s0 with equi-probability among 14 states\n",
    "        current_state = f\"s{choice(range(1,self.non_terminal_states_cnt+1), 1)[0]}\"\n",
    "        \n",
    "        to_return = [current_state]\n",
    "        while current_state != self.terminal_state_key:\n",
    "            next_states_prob = list(zip(*self.policy[current_state]))\n",
    "            current_state = choice(a=next_states_prob[0], p=next_states_prob[1], size=1)[0]\n",
    "            to_return.append(current_state)\n",
    "        if reward:\n",
    "            return self.generate_reward(to_return)\n",
    "        return to_return \n",
    "    \n",
    "    def get_states(self):\n",
    "        return list(self.policy.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo():\n",
    "    def __init__(self, states, first_visit=True):\n",
    "        self.states = states\n",
    "        self.v_pi_with_cnt = {}\n",
    "        self.reset_state_v_pi()\n",
    "        self.first_visit = first_visit\n",
    "        \n",
    "    def reset_state_v_pi(self):\n",
    "        for state in self.states:\n",
    "            self.v_pi_with_cnt[state] = (None, 0)\n",
    "            \n",
    "    @staticmethod\n",
    "    def retain_first_visits(state_reward_list):\n",
    "        seen = set()\n",
    "        return [(state, reward) for state, reward in state_reward_list\n",
    "                if not (state in seen or seen.add(state))]\n",
    "            \n",
    "    def update_vpi(self, state_reward_list):\n",
    "        if self.first_visit:\n",
    "            state_reward_list = self.retain_first_visits(state_reward_list)\n",
    "        for state, reward in state_reward_list:\n",
    "            current_v_pi, cnt = self.v_pi_with_cnt[state]\n",
    "            if current_v_pi is None:\n",
    "                current_v_pi = 0\n",
    "            cnt += 1\n",
    "            self.v_pi_with_cnt[state] = (current_v_pi+((reward - current_v_pi)/cnt), cnt) \n",
    "            \n",
    "    def return_v_pi(self):\n",
    "        return [(state,self.v_pi_with_cnt[state][0]) for state in self.states]\n",
    "    \n",
    "    def __str__(self):\n",
    "        to_return = \"0\".ljust(9,\" \")\n",
    "        print_cnt = 1\n",
    "        for state, v_pi in self.return_v_pi():\n",
    "            prefix = \"\\n\" if print_cnt%4 == 0 else \"\"\n",
    "            to_return += (f\"{prefix}{round(v_pi,3)}\".ljust(10,\" \"))\n",
    "            print_cnt += 1\n",
    "        return to_return+\"0\".ljust(9,\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld(policy)  # Policy from first section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_cnt = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_visit_exp = MonteCarlo(grid_world.get_states(), first_visit=True)\n",
    "every_visit_exp = MonteCarlo(grid_world.get_states(), first_visit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(episodes_cnt):\n",
    "    episode = grid_world.generate_episode(reward=True)  # Generate episode\n",
    "    first_visit_exp.update_vpi(episode)  # Update Vpi in first-visit\n",
    "    every_visit_exp.update_vpi(episode)  # Update Vpi in every-visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of first_visit_method:\n",
      "0        -0.805    -2.507    -3.458    \n",
      "-0.997   -2.52     -3.263    -2.603    \n",
      "-2.573   -3.382    -2.481    -1.099    \n",
      "-3.833   -2.695    -1.007    0        \n"
     ]
    }
   ],
   "source": [
    "print(\"Result of first_visit_method:\")\n",
    "print(first_visit_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of every_visit_method:\n",
      "0        -0.856    -2.434    -3.312    \n",
      "-1.016   -2.471    -3.243    -2.469    \n",
      "-2.536   -3.388    -2.433    -1.052    \n",
      "-3.73    -2.616    -0.976    0        \n"
     ]
    }
   ],
   "source": [
    "print(\"Result of every_visit_method:\")\n",
    "print(every_visit_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
